{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NanoPrompt/AI-Chatobots/blob/main/Agentic_Clinical_Trial_RAG_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from typing import TypedDict, List, Annotated\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_openai import ChatOpenAI # Using OpenAI for demonstration, easily replaceable with Gemini\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# --- 1. Define the Graph State ---\n",
        "# The state is the central object passed between all nodes in the graph.\n",
        "# 'messages' holds the conversation history and tool outputs.\n",
        "# 'analysis_report' is where structured, tool-generated analysis will be stored.\n",
        "class ClinicalTrialState(TypedDict):\n",
        "    \"\"\"Represents the state of our graph for the clinical trial analysis.\"\"\"\n",
        "    messages: Annotated[List[dict], lambda x, y: x + y] # The main message history\n",
        "    analysis_report: str # Dedicated field for structured data analysis results\n",
        "    question: str # The initial user question\n",
        "\n",
        "# --- 2. Define Specialized Tools (The \"Tool Plane\") ---\n",
        "\n",
        "# Placeholder for a Vector Store Retriever (e.g., Chroma, Qdrant, or GCP Vector Search)\n",
        "# In a real scenario, this would connect to a vector DB containing PolyNovo trial documents\n",
        "# (protocols, patient data summaries, adverse event reports).\n",
        "def get_retriever():\n",
        "    # Placeholder function for RAG retrieval\n",
        "    # In a production environment, this would:\n",
        "    # 1. Initialize an embedding model (e.g., text-embedding-004)\n",
        "    # 2. Connect to the clinical vector database\n",
        "    # 3. Perform a similarity search\n",
        "\n",
        "    # We simulate retrieval of relevant PolyNovo data\n",
        "    mock_clinical_data = {\n",
        "        \"PolyNovo_PN1_Protocol\": \"The primary endpoint is 75% complete wound closure by Day 60. The trial is phase IIb.\",\n",
        "        \"Patient_Cohort_Summary\": \"Initial patient cohort size was 50. Mean age 62. Primary cause of wound was burn injury (70%).\",\n",
        "        \"Adverse_Events\": \"Three Grade 2 adverse events were recorded: localized infection (2), device migration (1). All resolved.\"\n",
        "    }\n",
        "\n",
        "    def retrieve(query: str) -> str:\n",
        "        \"\"\"Retrieves specific clinical trial data from the PolyNovo document corpus.\"\"\"\n",
        "        if \"endpoint\" in query.lower() or \"closure\" in query.lower():\n",
        "            return f\"Retrieved Document: {mock_clinical_data['PolyNovo_PN1_Protocol']}\"\n",
        "        if \"patient\" in query.lower() or \"demographics\" in query.lower():\n",
        "            return f\"Retrieved Document: {mock_clinical_data['Patient_Cohort_Summary']}\"\n",
        "        if \"adverse\" in query.lower() or \"safety\" in query.lower():\n",
        "            return f\"Retrieved Document: {mock_clinical_data['Adverse_Events']}\"\n",
        "        return \"No specific, high-relevance document was found. The agent should rely on general knowledge or ask to refine the query.\"\n",
        "    return retrieve\n",
        "\n",
        "@tool\n",
        "def retrieve_clinical_data(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Retrieves relevant clinical trial documents (e.g., protocols, patient summaries,\n",
        "    safety data) from the PolyNovo vector database based on the query.\n",
        "    \"\"\"\n",
        "    return get_retriever()(query)\n",
        "\n",
        "@tool\n",
        "def analyze_metrics(data_summary: str) -> str:\n",
        "    \"\"\"\n",
        "    Performs specialized statistical analysis on structured clinical data summaries,\n",
        "    such as calculating efficacy rates, confidence intervals, or performing survival analysis.\n",
        "\n",
        "    Input must be a concise summary of the data extracted from documents.\n",
        "    \"\"\"\n",
        "    # This simulates a complex statistical routine (e.g., using pandas/scikit-learn/R)\n",
        "    if \"75% complete wound closure\" in data_summary and \"50\" in data_summary:\n",
        "        # Placeholder for a complex statistical model running on extracted data\n",
        "        mock_analysis_result = {\n",
        "            \"primary_endpoint_success_rate\": \"68%\",\n",
        "            \"confidence_interval_95\": \"[54%, 82%]\",\n",
        "            \"survival_median_days\": \"180 days (placeholder)\",\n",
        "            \"key_finding\": \"The observed primary endpoint success rate (68%) was slightly below the 75% target.\"\n",
        "        }\n",
        "        return f\"Specialized Analysis Results (JSON format):\\n{json.dumps(mock_analysis_result, indent=2)}\"\n",
        "    return \"Analysis tool executed, but insufficient structured data was provided for a comprehensive statistical report.\"\n",
        "\n",
        "tools = [retrieve_clinical_data, analyze_metrics]\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "\n",
        "# --- 3. Define the LLM/Agent Node ---\n",
        "\n",
        "# Initialize the LLM (Using a placeholder for the Gemini API call)\n",
        "# NOTE: In the live environment, you would use a dedicated API client for Gemini.\n",
        "# For LangChain/LangGraph, we can simulate the call structure with OpenAI's client.\n",
        "# Replace with the actual Gemini API integration when deployed:\n",
        "# model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-09-2025\")\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n",
        "\n",
        "def run_agent(state: ClinicalTrialState):\n",
        "    \"\"\"\n",
        "    The Agent (LLM) decides whether to use a tool or generate the final response.\n",
        "    It's the core reasoning engine of the pipeline.\n",
        "    \"\"\"\n",
        "    # The agent receives the whole message history, including previous tool outputs\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Add a system message to guide the LLM's role and purpose\n",
        "    system_message = SystemMessage(content=(\n",
        "        \"You are an expert Clinical Data Analyst for PolyNovo. Your goal is to accurately \"\n",
        "        \"analyze and report on clinical trial data. You have access to specialized tools: \"\n",
        "        \"'retrieve_clinical_data' for finding documents and 'analyze_metrics' for running \"\n",
        "        \"complex statistical routines on the data you retrieve. \"\n",
        "        \"Use the tools iteratively until you have enough information to generate a comprehensive, \"\n",
        "        \"grounded final report. If a tool returns results, call the 'analyze_metrics' tool next \"\n",
        "        \"if appropriate data is retrieved.\"\n",
        "    ))\n",
        "\n",
        "    # Combine system message and chat history for the prompt\n",
        "    full_prompt = [system_message] + messages\n",
        "\n",
        "    # Invoke the model (the agent decides on the next action: tool call or final response)\n",
        "    response = model.invoke(full_prompt)\n",
        "\n",
        "    # Store the response (which might contain a tool call) back in the state\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "# --- 4. Define Conditional Edges (Routing Logic) ---\n",
        "\n",
        "def should_continue(state: ClinicalTrialState) -> str:\n",
        "    \"\"\"\n",
        "    Conditional logic to determine the next step in the graph.\n",
        "    - If the LLM output includes a tool call, execute the tool.\n",
        "    - Otherwise, the LLM has generated the final answer, so end the graph.\n",
        "    \"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "\n",
        "    # Check if the last message contains a tool call (meaning the agent wants to use a tool)\n",
        "    if not isinstance(last_message, AIMessage):\n",
        "        # This shouldn't happen, but good practice to check\n",
        "        return END\n",
        "\n",
        "    if last_message.tool_calls:\n",
        "        print(\"--- ROUTER: Tool Call Detected. Proceeding to Tool Execution. ---\")\n",
        "        return \"call_tool\"\n",
        "    else:\n",
        "        print(\"--- ROUTER: Final Answer Generated. Ending Pipeline. ---\")\n",
        "        return END\n",
        "\n",
        "\n",
        "# --- 5. Build the LangGraph Workflow ---\n",
        "\n",
        "# Instantiate the graph with our defined state\n",
        "workflow = StateGraph(ClinicalTrialState)\n",
        "\n",
        "# Add Nodes\n",
        "workflow.add_node(\"agent\", run_agent)\n",
        "workflow.add_node(\"call_tool\", tool_node)\n",
        "\n",
        "# Set the Entry Point (The flow always starts with the Agent)\n",
        "workflow.add_edge(START, \"agent\")\n",
        "\n",
        "# Set Conditional Edges\n",
        "# After the agent runs, check if it wants to use a tool or end.\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"call_tool\": \"call_tool\", # If tool is called, go to the tool node\n",
        "        END: END                # If not, end the graph\n",
        "    }\n",
        ")\n",
        "\n",
        "# Set Normal Edges\n",
        "# After a tool runs, the output is fed back to the agent for further reasoning.\n",
        "workflow.add_edge(\"call_tool\", \"agent\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n",
        "\n",
        "# --- 6. Example Execution ---\n",
        "\n",
        "# Define the initial user question (This is the input to the pipeline)\n",
        "initial_question = \"What was the primary endpoint success rate for the PolyNovo PN1 clinical trial, considering the safety profile?\"\n",
        "initial_state = {\n",
        "    \"question\": initial_question,\n",
        "    \"messages\": [HumanMessage(content=initial_question)],\n",
        "    \"analysis_report\": \"\"\n",
        "}\n",
        "\n",
        "print(f\"\\n--- STARTING PIPELINE for Query: {initial_question} ---\\n\")\n",
        "\n",
        "# Run the pipeline\n",
        "final_state = None\n",
        "for step in app.stream(initial_state):\n",
        "    print(f\"Current Node Output: {list(step.keys())[0]}\")\n",
        "    final_state = step\n",
        "\n",
        "# Extract the final response\n",
        "if final_state and final_state.get('agent'):\n",
        "    final_message = final_state['agent']['messages'][-1].content\n",
        "    print(\"\\n--- FINAL REPORT ---\\n\")\n",
        "    print(final_message)\n",
        "else:\n",
        "    print(\"\\n--- PIPELINE COMPLETED WITHOUT FINAL AGENT MESSAGE ---\")\n",
        "\n",
        "# You would also typically persist the conversation state using a checkpointer\n",
        "# For example: app = workflow.compile(checkpointer=MemorySaver())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "LcGjMj7M0Hcv"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}